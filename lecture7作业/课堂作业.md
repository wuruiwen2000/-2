### 练习nltk和jieba分词的使用。  
### 制作中英文双语数据10对。  
### 将上述数据进行分词。  
### 创建自定义用户词典，模拟专业术语（术语库）。  
### 使用加载自定义用户词典后的分词器对数据再进行分词。  
#### 步骤如下：
打开cmd，输入pip install jieba
之后打开Python，输入如下代码：

#encoding=utf-8
import jieba
 
#全模式
text = "我来到北京清华大学"  
seg_list = jieba.cut(text, cut_all=True)  
print("Full Mode:", "/ ".join(seg_list)  )   

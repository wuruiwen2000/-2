### 练习nltk和jieba分词的使用。  
### 制作中英文双语数据10对。  
### 将上述数据进行分词。  
### 创建自定义用户词典，模拟专业术语（术语库）。  
### 使用加载自定义用户词典后的分词器对数据再进行分词。  
#### 制作中英文双语数据10对;将上述数据进行分词。步骤如下：
打开cmd，输入pip install jieba
之后打开Python，输入如下代码：

#encoding=utf-8
import jieba
 
#全模式  
text = "我来到北京清华大学"  
seg_list = jieba.cut(text, cut_all=True)  
print("Full Mode:", "/ ".join(seg_list)  )   

#精确模式  
seg_list = jieba.cut(text, cut_all=False)  
print("Exact Mode:", "/ ".join(seg_list)  )   
  

#默认是精确模式  
seg_list = jieba.cut(text)  
print("Default Mode:", "/ ".join(seg_list)  )   

#新词识别 “杭研”并没有在词典中,但是也被Viterbi算法识别出来了  
seg_list = jieba.cut("他来到了网易杭研大厦")   
print(", ".join(seg_list) )  


#搜索引擎模式  
seg_list = jieba.cut_for_search(text)   
print(", ".join(seg_list) )  

#### 创建自定义用户词典，模拟专业术语（术语库）;使用加载自定义用户词典后的分词器对数据再进行分词。步骤如下：
使用自定义词典

可以指定自己自定义的词典，以便包含jieba词库里没有的词。

虽然jieba有新词识别能力，但是自行添加新词可以保证更高的正确率 。

注意： 自定义词典不要用Windows记事本保存，这样会加入BOM标志，导致第一行的词被误读。

词典格式：一个词占一行；每一行分三部分，一部分为词语，一部分为词频，最后为词性（可省略），用空格隔开

验证下来，这里词典里面的词频主要是是为解决歧义而设置的，用于计算成词的组合概率
#encoding=utf-8  
import sys  
sys.path.append("../")  
import jieba   
import jieba.posseg as pseg  
  
test_sent = "李小福是创新办主任也是云计算方面的专家;"  
test_sent += "例如我输入一个带“韩玉赏鉴”的标题，在自定义词库中也增加了此词为N类型，python 的正则表达式是好用的"  
print("\n====下面结果未自定义词典====")
words = jieba.cut(test_sent) 
print("Result words:", "/ ".join(words)  )    
print("\n====下面是自定义userdict分词====")
jieba.load_userdict("C:/Users/Administrator/Desktop/userdict.txt")
words = jieba.cut(test_sent) 
print("Result words:", "/ ".join(words)  )   
jieba.load_userdict(userdict.txt)



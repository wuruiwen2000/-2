### 练习nltk和jieba分词的使用。  
### 制作中英文双语数据10对。  
### 将上述数据进行分词。  
### 创建自定义用户词典，模拟专业术语（术语库）。  
### 使用加载自定义用户词典后的分词器对数据再进行分词。  
#### 步骤如下：
打开cmd，输入pip install jieba
之后打开Python，输入如下代码：

#encoding=utf-8
import jieba
 
#全模式
text = "我来到北京清华大学"  
seg_list = jieba.cut(text, cut_all=True)  
print("Full Mode:", "/ ".join(seg_list)  )   

#精确模式
seg_list = jieba.cut(text, cut_all=False)
print("Exact Mode:", "/ ".join(seg_list)  ) 


#默认是精确模式
seg_list = jieba.cut(text)
print("Default Mode:", "/ ".join(seg_list)  ) 

#新词识别 “杭研”并没有在词典中,但是也被Viterbi算法识别出来了
seg_list = jieba.cut("他来到了网易杭研大厦") 
print(", ".join(seg_list) )


#搜索引擎模式
seg_list = jieba.cut_for_search(text) 
print(", ".join(seg_list) )
